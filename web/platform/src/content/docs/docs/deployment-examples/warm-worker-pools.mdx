---
title: "Warm Worker Pools"
description: "Configure high-performance warm worker pools for Java, TypeScript, and other JIT-compiled languages"
pagefind: true
---


## Overview

Warm worker pools are a performance optimization feature in NativeLink that dramatically reduces build times for languages with slow cold-start characteristics (Java, TypeScript, etc.) by maintaining pools of pre-warmed worker containers.

:::caution[Platform Requirements]
Warm worker pools require **Linux/Unix systems** with CRI-O installed. This feature uses Unix domain sockets for container runtime communication and **isn't available on Windows**.
:::

### Key Benefits

- **60-80% faster builds** for Java, TypeScript, and other JIT-compiled languages
- **Consistent performance** across repeated builds
- **Automatic worker recycling** to prevent memory leaks
- **Zero configuration for standard use cases** - just enable and go

### How It Works

Instead of starting a fresh container for each build (cold start):
```
Cold Start: 30-45 seconds
  ├─ Container creation: 2-3s
  ├─ JVM initialization: 10-15s
  ├─ Class loading: 10-15s
  └─ JIT warmup: 5-10s
```

Warm worker pools keep containers running and warmed up:
```
Warm Start: 50-100ms
  └─ Acquire from pool: 50-100ms
```

## When to Use Warm Worker Pools

### ✅ Use Warm Pools For:

- **Java/JVM builds** (Java, Kotlin, Scala, Groovy)
- **TypeScript/JavaScript builds** (especially with large codebases)
- **Repeated builds** (CI/CD pipelines with frequent builds)
- **Large mono repos** with hundreds of targets

### ❌ Don't Use Warm Pools For:

- **Native compiled languages** (C, C++, Rust, Go) - already fast cold starts
- **Infrequent builds** (waste of resources if pool sits idle)
- **Memory-constrained environments** (warm workers consume memory)

## Configuration

### Basic Setup

Add a `warm_worker_pools` section to your scheduler configuration:

```json5
{
  schedulers: {
    main: {
      simple: {
        // Your existing scheduler config...
        supported_platform_properties: {
          lang: "exact",
        },

        // Enable warm worker pools
        warm_worker_pools: {
          pools: [
            {
              name: "java-pool",
              language: "jvm",
              cri_socket: "unix:///var/run/crio/crio.sock",
              container_image: "ghcr.io/tracemachina/nativelink-worker-java:latest",
              min_warm_workers: 5,
              max_workers: 50,
              warmup: {
                commands: [
                  { argv: ["/opt/warmup/jvm-warmup.sh"], timeout_s: 60 }
                ],
              },
              lifecycle: {
                worker_ttl_seconds: 3600,
                max_jobs_per_worker: 200,
              },
            },
          ],
        },
      },
    },
  },
}
```

### Configuration Options

#### Pool Configuration

| Field | Required | Default | Description |
|-------|----------|---------|-------------|
| `name` | Yes | - | Pool identifier (for example, "java-pool") |
| `language` | Yes | - | Runtime type: `jvm`, `nodejs`, or custom |
| `cri_socket` | Yes | - | Path to CRI-O Unix socket |
| `container_image` | Yes | - | Docker image for workers |
| `min_warm_workers` | No | 2 | Minimum workers to keep warm |
| `max_workers` | No | 20 | Maximum pool size |

#### Warmup Configuration

| Field | Required | Default | Description |
|-------|----------|---------|-------------|
| `commands` | No | [] | Commands to run on container start |
| `post_job_cleanup` | No | [] | Commands to run after each job |

Example warmup for Java:
```json5
warmup: {
  commands: [
    // Warm up JVM with compilation workload
    { argv: ["/opt/warmup/jvm-warmup.sh"], timeout_s: 60 }
  ],
  post_job_cleanup: [
    // Force GC after each job to prevent memory leaks
    { argv: ["jcmd", "1", "GC.run"], timeout_s: 30 }
  ],
}
```

#### Lifecycle Configuration

| Field | Required | Default | Description |
|-------|----------|---------|-------------|
| `worker_ttl_seconds` | No | 3600 | Max worker lifetime (prevents memory leaks) |
| `max_jobs_per_worker` | No | 200 | Recycle worker after N jobs |
| `gc_job_frequency` | No | 25 | Run GC every N jobs |

## Security: Job Isolation

### ⚠️ Important: Multi-Tenant Security Considerations

By default, warm workers **reuse container state across multiple jobs** (up to 200 jobs per worker). This provides excellent performance but creates potential **cross-tenant contamination risks** in shared RBE deployments.

**Default behavior without isolation:**
```
Worker Lifecycle (WITHOUT isolation):
  Worker boots → Job 1 → Job 2 → ... → Job 200 → Worker recycled
  ⚠️ Shared: filesystem, memory, environment variables, temp files
```

**Risk:** Secrets, artifacts, or state from one tenant's build could leak to another tenant's build.

### Copy-on-Write (COW) Isolation

To prevent state leakage, enable **OverlayFS isolation** in production deployments:

```json5
{
  warmup: { /* ... */ },
  lifecycle: { /* ... */ },

  // Enable isolation (RECOMMENDED for production)
  isolation: {
    strategy: "overlayfs",
    template_cache_path: "/var/lib/nativelink/warm-templates",
    job_workspace_path: "/var/lib/nativelink/warm-jobs",
  },
}
```

**How it works:**
```
Template Creation (once):
  Boot worker → Warmup JVM/Node → Save as template → Reuse forever

Job Execution (per job):
  Clone template (COW) → Execute job in isolation → Cleanup → Repeat
  ✅ Isolated: Each job gets fresh filesystem layer
  ✅ Fast: Template cloning ~250ms (vs 30-45s cold start)
  ✅ Secure: No cross-job contamination
```

### When to Enable Isolation

| Deployment Type | Isolation Recommended? | Reason |
|----------------|----------------------|---------|
| **Multi-tenant RBE** | ✅ **Required** | Different companies/teams sharing infrastructure |
| **Single-tenant production** | ✅ **Recommended** | Defense-in-depth for different projects |
| **Development/testing** | ⚠️ Optional | Lower risk, can trade security for simplicity |
| **Local builds** | ❌ Not applicable | Warm pools don't work locally |

### Configuration Options

| Field | Default | Description |
|-------|---------|-------------|
| `strategy` | `"none"` | `"none"` (shared state), `"overlayfs"` (COW isolation) |
| `template_cache_path` | `/var/lib/nativelink/warm-templates` | Where template snapshots are stored |
| `job_workspace_path` | `/var/lib/nativelink/warm-jobs` | Where ephemeral job workspaces are created |

**Note:** `strategy: "none"` maintains backward compatibility with existing configurations.

### Performance Impact

Isolation adds minimal overhead compared to cold starts:

| Metric | Without Isolation | With OverlayFS Isolation | Cold Start |
|--------|------------------|------------------------|------------|
| First job | ~100ms | ~250ms (template creation) | 30-45s |
| Subsequent jobs | ~100ms | ~250ms (clone + cleanup) | 30-45s |
| **Slowdown** | - | +150ms | - |

**Verdict:** 150ms overhead is negligible compared to 30-45s saved vs cold starts.

## Routing Actions to Warm Pools

NativeLink automatically routes actions to warm pools based on **platform properties**. You don't need to modify your build files in most cases.

### Automatic Detection

The scheduler uses these heuristics to detect language:

1. **Platform property `lang`**:
   ```python
   java_binary(
       exec_properties = {"lang": "java"},
   )
   ```

2. **Platform property `toolchain`** (Bazel sets this automatically):
   ```python
   # Bazel automatically sets toolchain for java_binary, scala_binary, etc.
   java_binary(name = "myapp")  # Automatically routed to java-pool
   ```

3. **Platform property `Pool`** (Bazel exec groups):
   ```python
   genrule(
       exec_properties = {"Pool": "java-worker-pool"},
   )
   ```

### Supported Language Mappings

| Platform Property Value | Routes To Pool |
|------------------------|---------------|
| `lang=java`, `lang=jvm`, `lang=kotlin`, `lang=scala` | `java-pool` |
| `lang=typescript`, `lang=ts`, `lang=javascript`, `lang=node` | `typescript-pool` |
| `toolchain` containing `java` or `jvm` | `java-pool` |
| `toolchain` containing `node` or `typescript` | `typescript-pool` |
| `Pool` containing `java` | `java-pool` |
| `Pool` containing `node` or `typescript` | `typescript-pool` |

### Manual Routing

If automatic detection doesn't work, explicitly set the `lang` property:

```python
# In your BUILD file
java_library(
    name = "mylib",
    exec_properties = {
        "lang": "java",  # Explicitly route to java-pool
    },
)
```

Or via `.bazelrc`:
```bash
# Route all Java targets to warm pool
build --java_runtime_version=remotejdk_11
build --tool_java_runtime_version=remotejdk_11
build --action_env=BAZEL_DO_NOT_DETECT_CPP_TOOLCHAIN=1
build --experimental_remote_execution_keepalive=true
build --remote_default_exec_properties=lang=java
```

## Prerequisites

### CRI-O Installation

Warm worker pools require CRI-O (Container Runtime Interface):

**Ubuntu/Debian:**
```bash
# Add CRI-O repository
OS=xUbuntu_22.04
VERSION=1.28

echo "deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /" \
  | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list

curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key \
  | sudo apt-key add -

# Install CRI-O
sudo apt-get update
sudo apt-get install -y cri-o cri-tools

# Start CRI-O
sudo systemctl enable crio
sudo systemctl start crio

# Verify installation
sudo crictl version
```

**Other systems:** See [CRI-O installation guide](https://github.com/cri-o/cri-o/blob/main/install.md)

### Worker Images

You need container images with your build tools pre-installed:

**Option 1: Use pre-built images** (coming soon):
```bash
docker pull ghcr.io/tracemachina/nativelink-worker-java:latest
docker pull ghcr.io/tracemachina/nativelink-worker-node:latest
```

**Option 2: Build custom images:**

Create a `Dockerfile`:
```dockerfile
FROM ubuntu:22.04

# Install Java
RUN apt-get update && apt-get install -y \
    openjdk-17-jdk \
    maven \
    gradle

# Install NativeLink worker (placeholder - adjust for your setup)
COPY nativelink-worker /usr/local/bin/

# Add warmup script
COPY jvm-warmup.sh /opt/warmup/
RUN chmod +x /opt/warmup/jvm-warmup.sh

# Keep container running
CMD ["/bin/bash", "-c", "sleep infinity"]
```

Example warmup script (`jvm-warmup.sh`):
```bash
#!/bin/bash
# Warm up JVM by compiling and running a simple program
echo "public class Warmup { public static void main(String[] args) { for(int i=0; i<10000; i++) { String s = new String(\"test\" + i); } } }" > Warmup.java
javac Warmup.java
for i in {1..100}; do
  java Warmup
done
rm Warmup.java Warmup.class
```

Build and make available to CRI-O:
```bash
docker build -t nativelink-worker-java:latest .
# CRI-O can pull from Docker daemon
```

## Monitoring

### Logs

Check scheduler logs for warm pool activity:

```bash
# Initialization
INFO Initializing warm worker pools (defined in config) pools=2

# Successful initialization
INFO Warm worker pools initialized successfully

# Action routing
DEBUG Acquired warm worker from pool (skipping standard worker lookup) pool_name="java-pool"

# Fallback to standard workers
WARN Failed to acquire warm worker, falling back to standard workers pool_name="java-pool"
```

### Metrics (Coming Soon)

Future releases will include Prometheus metrics:

```
# Pool health
warm_pool_ready_workers{pool="java-pool"} 5
warm_pool_active_workers{pool="java-pool"} 3
warm_pool_provisioning_workers{pool="java-pool"} 1

# Performance
warm_pool_acquisition_duration_seconds{pool="java-pool",quantile="0.99"} 0.05
warm_pool_job_duration_seconds{pool="java-pool",quantile="0.99"} 2.1

# Utilization
warm_pool_acquisitions_total{pool="java-pool"} 1523
warm_pool_acquisition_failures_total{pool="java-pool"} 12
```

## Troubleshooting

### Workers Not Being Used

**Problem:** Builds still slow, logs show "falling back to standard workers"

**Solutions:**

1. **Check platform properties:**
   ```bash
   # Run build with debug logging
   bazel build --remote_executor=grpc://localhost:50051 \
               --execution_log_json_file=exec.log \
               //your:target

   # Check if lang property is set
   jq '.[] | select(.type=="spawn") | .executionPlatform.properties' exec.log
   ```

2. **Explicitly set lang property:**
   ```bash
   # In .bazelrc
   build --remote_default_exec_properties=lang=java
   ```

3. **Check CRI-O connectivity:**
   ```bash
   sudo crictl --runtime-endpoint unix:///var/run/crio/crio.sock ps
   ```

### Pool Manager Not Initializing

**Problem:** Logs show "Failed to initialize warm worker pool manager"

**Solutions:**

1. **Check CRI-O is running:**
   ```bash
   sudo systemctl status crio
   sudo systemctl start crio  # If not running
   ```

2. **Verify socket path:**
   ```bash
   ls -la /var/run/crio/crio.sock
   # Should show a socket file
   ```

3. **Check image availability:**
   ```bash
   sudo crictl images | grep nativelink-worker
   ```

4. **Check permissions:**
   ```bash
   # NativeLink process needs access to CRI-O socket
   sudo usermod -aG crio nativelink-user
   ```

### High Memory Usage

**Problem:** Worker containers consuming too much memory

**Solutions:**

1. **Reduce pool size:**
   ```json5
   min_warm_workers: 2,  // Reduce from 5
   max_workers: 10,      // Reduce from 50
   ```

2. **Increase GC frequency:**
   ```json5
   lifecycle: {
     gc_job_frequency: 10,  // Run GC every 10 jobs instead of 25
   }
   ```

3. **Reduce worker TTL:**
   ```json5
   lifecycle: {
     worker_ttl_seconds: 1800,  // 30 minutes instead of 1 hour
     max_jobs_per_worker: 100,  // Recycle more frequently
   }
   ```

4. **Add post-job cleanup:**
   ```json5
   warmup: {
     post_job_cleanup: [
       { argv: ["jcmd", "1", "GC.run"] },  // Force GC after each job
     ],
   }
   ```

## Performance Expectations

### Typical Performance Improvements

Based on testing with real-world workloads:

| Language | Cold Start | Warm Start | Improvement |
|----------|-----------|-----------|-------------|
| Java | 30-45s | 2-3s | 85-93% faster |
| Kotlin | 35-50s | 2-4s | 88-94% faster |
| TypeScript | 15-25s | 1-2s | 87-93% faster |
| JavaScript | 10-15s | 0.5-1s | 90-95% faster |

### Build Time Comparison

Example: Large TypeScript monorepo (500 targets)

**Without warm pools:**
```
Total build time: 45 minutes
  - Container overhead: 15 minutes (500 targets × 30s each ÷ 10 parallel)
  - Actual compilation: 30 minutes
```

**With warm pools (10 workers):**
```
Total build time: 32 minutes  (28% faster)
  - Container overhead: 2 minutes (500 targets × 100ms each ÷ 10 parallel)
  - Actual compilation: 30 minutes
```

**Savings:** 13 minutes per full build

## Best Practices

### 1. Right-Size Your Pools

Start conservative and scale up:
```json5
// Start here
min_warm_workers: 2
max_workers: 10

// Scale to (if builds are fast and pool is saturated)
min_warm_workers: 5
max_workers: 50
```

### 2. Monitor and Tune TTL

Balance memory usage vs. performance:
```json5
// High-frequency builds (CI/CD running 24/7)
worker_ttl_seconds: 7200  // 2 hours

// Medium-frequency builds (business hours only)
worker_ttl_seconds: 3600  // 1 hour

// Low-frequency builds (occasional)
worker_ttl_seconds: 1800  // 30 minutes
```

### 3. Use Warmup Scripts

Invest time in good warmup scripts for best results:

**Bad warmup** (minimal benefit):
```bash
#!/bin/bash
java -version  # Just checks Java works
```

**Good warmup** (60-80% improvement):
```bash
#!/bin/bash
# Actually exercises JVM JIT compiler
javac Warmup.java
for i in {1..100}; do java Warmup; done
```

**Great warmup** (80-90% improvement):
```bash
#!/bin/bash
# Mimics actual build workload
javac -cp "lib/*" Sample.java
for i in {1..200}; do
  java -cp "lib/*:." Sample
done
# Pre-load common classes
java -Xshare:dump
```

### 4. Layer Your Images Efficiently

**Bad Dockerfile** (slow cold start):
```dockerfile
FROM ubuntu:22.04
RUN apt-get update && apt-get install -y openjdk-17-jdk maven
# Downloads every time
```

**Good Dockerfile** (fast cold start):
```dockerfile
FROM eclipse-temurin:17-jdk
# Pre-installed Java, optimized layers
COPY maven/ /opt/maven/
ENV PATH="/opt/maven/bin:$PATH"
# Pre-download common dependencies
COPY pom.xml /tmp/
RUN cd /tmp && mvn dependency:go-offline
```

### 5. Set Realistic max_workers

Consider your infrastructure capacity:
```
max_workers = (total_memory_GB - reserved_GB) / worker_memory_GB

Example:
  - Total memory: 64 GB
  - Reserved (OS, NativeLink, etc.): 16 GB
  - Worker memory: ~2 GB each
  - max_workers = (64 - 16) / 2 = 24 workers
```

## Cost Considerations

### Resource Usage

Each warm worker consumes:
- **Memory:** 1-4 GB (depends on workload)
- **CPU:** 0.1-0.5 cores (idle), 1+ cores (active)
- **Storage:** 1-5 GB (container image + cache)

### Cost-Benefit Analysis

**Scenario:** CI/CD running 100 builds/day on AWS

**Without warm pools:**
```
  - Build time: 45 min/build × 100 builds = 75 hours/day
  - Compute cost: 75h × $0.10/h = $7.50/day
  - Developer time wasted: 45 min × 100 = 75 hours/day
```

**With warm pools (10 workers):**
```
  - Build time: 32 min/build × 100 builds = 53 hours/day
  - Warm pool overhead: 10 workers × 24h × $0.02/h = $4.80/day
  - Compute cost: 53h × $0.10/h + $4.80 = $10.10/day
  - Developer time wasted: 32 min × 100 = 53 hours/day
```

**Result:**
- Compute cost: +$2.60/day (35% more)
- Time saved: 22 hours/day of build time
- **ROI:** Developers save 22 hours/day waiting for builds

## Migration Guide

### Step 1: Test in Dev Environment

1. Set up CRI-O on a test machine
2. Configure warm pools with small limits:
   ```json5
   min_warm_workers: 1
   max_workers: 2
   ```
3. Run sample builds and verify they use warm pools
4. Compare build times (cold vs. warm)

### Step 2: Production Deployment

1. Update NativeLink configuration to enable warm pools
2. Deploy with feature flag OFF initially:
   ```json5
   // Set min_warm_workers to 0 to disable pre-warming
   min_warm_workers: 0
   max_workers: 10
   ```
3. Monitor logs for errors
4. Gradually increase min_warm_workers:
   ```
   Day 1: min_warm_workers: 0 (disabled)
   Day 2: min_warm_workers: 1 (testing)
   Day 3: min_warm_workers: 2
   Day 7: min_warm_workers: 5 (full deployment)
   ```

### Step 3: Optimization

1. Monitor build times and pool utilization
2. Adjust pool size based on demand
3. Tune warmup scripts for your workload
4. Set appropriate TTL values

## FAQ

**Q: Do I need to modify my BUILD files?**
A: Usually no. Bazel automatically sets toolchain properties that NativeLink uses for routing.

**Q: What if a warm worker acquisition fails?**
A: NativeLink automatically falls back to standard workers. Your builds never fail due to warm pool issues.

**Q: Can I use warm pools with local builds?**
A: No, warm pools only work with remote execution. They require CRI-O which manages container lifecycle.

**Q: How many pools can I create?**
A: As many as you want, but typically you'll only need 2-3 (Java, TypeScript, maybe one custom).

**Q: Do warm pools work with Buck2/Goma?**
A: Not yet. Currently only Bazel remote execution is supported.

**Q: Can I use warm pools in self-hosted NativeLink?**
A: Yes! This feature works with both cloud and self-hosted deployments.

**Q: What's the cold start overhead if pools aren't initialized yet?**
A: First action of each type pays ~5-10s initialization cost, then subsequent actions are fast.

## Next Steps

- [Example Configurations](../deployment-examples/warm-worker-pools.json5)
- [Integration Summary](../SCHEDULER_INTEGRATION_SUMMARY.md)
- [Development Progress](../WARM_POOLS_PROGRESS.md)
- [NativeLink Documentation](https://nativelink.com/docs)

## Support

- **Slack:** [Join NativeLink Community](https://forms.gle/LtaWSixEC6bYi5xF7)
- **GitHub Issues:** [Report bugs or request features](https://github.com/TraceMachina/nativelink/issues)
- **Email:** support@nativelink.com (enterprise support)
